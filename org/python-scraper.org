#+OPTIONS: toc:nil
#+BEGIN_HTML
---
layout:     post
title:      Basing a scraper on PostgreSQL 9.5 'UPSERT'
categories: postgresql python
---
#+END_HTML

#+BEGIN_SRC ipython :session :results silent :exports none
import os
import inspect
from pprint import pprint

%load_ext autoreload
%autoreload 2

from upsert_scraper import scraper
from upsert_scraper.scraper import scrape

class fresh_test_database:
    def __enter__(self):
        import psycopg2
        dbname="python_scraper_test"
        !pkill $dbname
        conn = psycopg2.connect("dbname=postgres")
        conn.autocommit = True
        cur = conn.cursor()
        cur.execute("DROP DATABASE IF EXISTS %s" % dbname)
        cur.execute("CREATE DATABASE %s" % dbname)

        return "postgres://localhost/%s" % dbname
    def __exit__(self, type, value, traceback):
        return None

def print_source(obj, prefix=None):
    print("#+BEGIN_EXAMPLE python")
    if prefix:
        print(prefix)
    print(inspect.getsource(obj))
    print("#+END_EXAMPLE")

def scrape(body, timestamp=None, database_url=None):
    kwargs = dict(database_url=database_url)
    if timestamp:
        kwargs['timestamp'] = timestamp
    return scraper.scrape(body + '\n', **kwargs)

def print_query(index):
    with open('upsert_scraper/query.sql') as f:
        query = [line.strip() for line in f.read().split(';')]
        print("#+BEGIN_EXAMPLE sql")
        print(query[index])
        print("#+END_EXAMPLE")

#+END_SRC

My roommate asked me for advice on building a simple web scraper this
weekend. The script should run automatically and check the targeted website at
least every hour. My roommate is not in possession of a server and his laptop is
not always online. Within these constraints, we succeeded in building a tool
that checks every 10 minutes and has zero maintenance costs. In this post, I
will explain how we did it and used PostgreSQL's new UPSERT feature to our
advantage.

* Introduction

Given a website that presents the following source code to my roommate on
=2016-03-31=:

#+NAME: before
#+BEGIN_EXAMPLE html
<html>
  A thing
</html>
#+END_EXAMPLE

A couple of days later, the owners of this fictional website have added
something to their page. The source looks like this on =2016-04-02=:

#+NAME: after
#+BEGIN_EXAMPLE html
<html>
  A thing
  A new thing!
</html>
#+END_EXAMPLE

Keeping close track of these mutations has great value to my roommate. This is
why I think he would want the following diff (he did not know yet what a diff
was--just wanted a notification) in his mailbox after the change occured.

#+BEGIN_SRC ipython :session :var before=before after=after :results output raw :exports results
with fresh_test_database() as db:
    scrape(before, '2016-03-31', database_url=db)
    result = scrape(after, '2016-04-02', database_url=db)
for line in result:
    print(": " + line, end='')
#+END_SRC

#+RESULTS:
: *** 2016-03-31 00:00:00+02:00
: --- 2016-04-02 00:00:00+02:00
: ***************
: *** 1,3 ****
: --- 1,4 ----
:   <html>
:     A thing
: +   A new thing!
:   </html>

A push notification of the email would alarm him of the modification. He would
consequently rush to the website to do the thing that he needs to do with the
new thing.

* Objective

There are many ways to build a web scraper. In this post, I will show you how we
did it within the following constraints:

- Online 24/7
- Scrapes at least every hour
- No recurring costs (except development time, of course)
- Notify via email when the website has changed
- Extra: add a diff to the email
- Extra: save all the data so we could analyze it later
- Extra: use the new PostgreSQL 9.5 'UPSERT' feature

The source code for this post (Babel & Org-mode, IPython, PostgreSQL) and the
scraper (Python, PostgreSQL) is [[https://github.com/pepijn/pepijn.github.io/tree/master/org][available on GitHub]].

* The scraper

From now on, the snippets that follow are either IPython, or SQL code and/or
have their output underneath. The =before= and =after= variables that appear in the
code have the following values, based on the HTML snippets in the introduction:

#+BEGIN_SRC ipython :session :var before=before after=after :results output code :exports both
pprint(before)
pprint(after)
#+END_SRC

#+RESULTS:
#+BEGIN_SRC ipython
'<html>\n  A thing\n</html>'
'<html>\n  A thing\n  A new thing!\n</html>'
#+END_SRC

** Example scenarios

Before diving into the implementation details, let us look at two example
scenarios that one encounters while using a scraper.

*** Scrape after mutation

The first scenario involves two /scrapes/. The first one scrapes the =before=
content and the second one the =after= content. The output diff of the second
scrape is, as expected, present and is printed to the block under the code.

#+BEGIN_SRC ipython :session :var before=before after=after :results output code :exports both
with fresh_test_database() as db:
    scrape(before, '2016-03-31', database_url=db)
    pprint(scrape(after, '2016-04-02', database_url=db))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC ipython
['*** 2016-03-31 00:00:00+02:00\n',
 '--- 2016-04-02 00:00:00+02:00\n',
 '***************\n',
 '*** 1,3 ****\n',
 '--- 1,4 ----\n',
 '  <html>\n',
 '    A thing\n',
 '+   A new thing!\n',
 '  </html>\n']
#+END_SRC

*** Scrape unchanged content

This scenario scrapes the =after= content twice, in sequence. As a result, the
output of the third scrape (the second =after= scrape) has no output. This is
the how the scraper works; it can only return a diff when something has changed.

#+BEGIN_SRC ipython :session :var before=before after=after :results output code :exports both
with fresh_test_database() as db:
    scrape(before, '2015-01-01', database_url=db)
    scrape(after, '2016-01-01', database_url=db)
    pprint(scrape(after, '2016-04-03', database_url=db))
#+END_SRC

#+RESULTS:
#+BEGIN_SRC ipython
None
#+END_SRC

** Implementation

The entry point of the scraper is the =scrape()= function:

#+BEGIN_SRC ipython :session :results output raw :exports results
print_source(scraper.scrape)
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE python
def scrape(body, timestamp=None, database_url=None):
    assert timestamp
    path = os.path.join(os.path.dirname(__file__), 'query.sql')
    with open(path) as f:
        with psycopg2.connect(database_url) as conn:
            with conn.cursor() as cur:
                cur.execute(f.read(), (body, timestamp))
                results = cur.fetchall()
                return diff(*results)

#+END_EXAMPLE

*** PostgreSQL

Most of the interesting stuff happens in the =query.sql= file that is loaded by
the =scrape()= function.

**** Creating the table

#+BEGIN_SRC ipython :session :results output raw :exports results
print_query(0)
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE sql
CREATE TABLE IF NOT EXISTS scraps (
  id serial PRIMARY KEY,
  body text NOT NULL UNIQUE,
  seen_at timestamptz[] NOT NULL
)
#+END_EXAMPLE

**** Load cryptographic hash functions

#+BEGIN_SRC ipython :session :results output raw :exports results
print_query(1)
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE sql
CREATE EXTENSION IF NOT EXISTS pgcrypto
#+END_EXAMPLE

**** Insert or update ('UPSERT')

#+BEGIN_SRC ipython :session :results output raw :exports results
print_query(2)
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE sql
WITH body AS (SELECT %s::text AS txt)
INSERT INTO scraps (hash, body, seen_at)
    SELECT digest(txt, 'sha1'), txt, ARRAY[%s::timestamptz]
    FROM body
  ON CONFLICT (hash) DO UPDATE
    SET seen_at = scraps.seen_at || EXCLUDED.seen_at
#+END_EXAMPLE

**** Return the last record(s)

#+BEGIN_SRC ipython :session :results output raw :exports results
print_query(3)
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE sql
WITH last_observations AS (
       SELECT body, unnest(seen_at) AS seen_at
       FROM scraps
       ORDER BY seen_at DESC
       LIMIT 2),
     distinct_observations AS (
       SELECT DISTINCT ON (body) seen_at, body
       FROM last_observations)
SELECT *
FROM distinct_observations
ORDER BY seen_at DESC
#+END_EXAMPLE

*** Diffing

#+BEGIN_SRC ipython :session :results output raw :exports results
print_source(scraper.diff)
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE python
def diff(after, before=None):
    if not before:
        return None

    diff = difflib.context_diff(before[1].splitlines(True),
                                after[1].splitlines(True),
                                str(before[0]),
                                str(after[0]))
    return list(diff)

#+END_EXAMPLE


* Results

#+BEGIN_SRC ipython :session :var before=before after=after :exports both :results output
with fresh_test_database() as db:
    scrape(before, '2016-03-30', database_url=db)
    scrape(before, '2016-03-31', database_url=db)
    scrape(after, '2016-04-01', database_url=db)
    scrape(after, '2016-04-02', database_url=db)
    scrape("<h1>blocked</h1>", '2016-04-03', database_url=db)

    !echo scraps table: # prevents whitespace truncation
    !psql $db -c 'SELECT body, seen_at FROM scraps'
#+END_SRC


#+RESULTS:
#+begin_example
scraps table:
       body       |                       seen_at
------------------+-----------------------------------------------------
 <html>          +| {"2016-03-30 00:00:00+02","2016-03-31 00:00:00+02"}
   A thing       +|
 </html>         +|
                  |
 <html>          +| {"2016-04-01 00:00:00+02","2016-04-02 00:00:00+02"}
   A thing       +|
   A new thing   +|
 </html>         +|
                  |
 <h1>blocked</h1>+| {"2016-04-03 00:00:00+02"}
                  |
(3 rows)

#+end_example

#+BEGIN_SRC ipython :session :results output raw :exports results
print_source(scraper.main)
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE python
def main():
    database_url = os.environ['DATABASE_URL']
    sender = os.environ['FROM']
    sendgrid_user = os.environ['SENDGRID_USERNAME']
    sendgrid_pass = os.environ['SENDGRID_PASSWORD']

    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--recipients', required=True)
    parser.add_argument('--subject', required=True)
    args = parser.parse_args()

    import sys
    body = sys.stdin.read()

    diff = scrape(body, database_url=database_url)

    if not diff:
        return

    import sendgrid
    sg = sendgrid.SendGridClient(sendgrid_user, sendgrid_pass)
    message = sendgrid.Mail(to=args.recipients,
                            subject=args.subject,
                            text=''.join(diff),
                            from_email=sender)
    status, msg = sg.send(message)
    if status is not 200:
        sys.exit(1)

#+END_EXAMPLE

#+BEGIN_SRC sh :results verbatim :cache yes :exports code :var APP_NAME="vastgoed-scrapert"
cd upsert_scraper
heroku destroy $APP_NAME --confirm $APP_NAME

set -eu
heroku create $APP_NAME --region eu

heroku config:set TZ=Europe/Amsterdam
heroku config:set FROM=scraper@example.com

heroku addons:create heroku-postgresql:hobby-dev --version 9.5
heroku addons:create sendgrid:starter

heroku addons:create scheduler:standard
heroku addons:open scheduler

git push heroku master
#+END_SRC

#+RESULTS[c0a5b57e21ee8acf074084920ab33f00f459367b]:
#+begin_example
https://vastgoed-scrapert.herokuapp.com/ | https://git.heroku.com/vastgoed-scrapert.git
TZ: Europe/Amsterdam
FROM: scraper@example.com
Creating postgresql-graceful-65275... done, (free)
Adding postgresql-graceful-65275 to vastgoed-scrapert... done
Setting DATABASE_URL and restarting vastgoed-scrapert... done, v5
Database has been created and is available
 ! This database is empty. If upgrading, you can transfer
 ! data from another database with pg:copy
Use `heroku addons:docs heroku-postgresql` to view documentation.
Creating sendgrid-horizontal-56807... done, (free)
Adding sendgrid-horizontal-56807 to vastgoed-scrapert... done
Setting SENDGRID_PASSWORD, SENDGRID_USERNAME and restarting vastgoed-scrapert... done, v6
Use `heroku addons:docs sendgrid` to view documentation.
Creating scheduler-trapezoidal-14878... done, (free)
Adding scheduler-trapezoidal-14878 to vastgoed-scrapert... done
This add-on consumes dyno hours, which could impact your monthly bill. To learn more:
http://devcenter.heroku.com/addons_with_dyno_hour_usage

To manage scheduled jobs run:
heroku addons:open scheduler
Use `heroku addons:docs scheduler` to view documentation.
Opening https://addons-sso.heroku.com/apps/vastgoed-scrapert/addons/d33068db-fcf8-44a6-a2f1-872b2e817661...
#+end_example

* Conclusion

* Acknowledgements

- IPython
- gregsexton/ob-ipython
- Babel (Org-mode)

* Appendix

** Creating a fresh test database

#+BEGIN_SRC ipython :session :results output raw :exports results
print_source(fresh_test_database.__enter__,
             prefix="class fresh_test_database():")
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE python
class fresh_test_database():
    def __enter__(self):
        import psycopg2
        dbname="python_scraper_test"
        get_ipython().system('pkill $dbname')
        conn = psycopg2.connect("dbname=postgres")
        conn.autocommit = True
        cur = conn.cursor()
        cur.execute("DROP DATABASE IF EXISTS %s" % dbname)
        cur.execute("CREATE DATABASE %s" % dbname)

        return "postgres://localhost/%s" % dbname

#+END_EXAMPLE
